input {
	beats {
		port => 5044
	}

	tcp {
		port => 50000
		codec => json_lines {
			target => "[document]"
		}
	}
}

# log like 2024-03-20 06:40:09,377 DEBUG [Generator] issue event file KS_batch DONE
# 解析为 timestamp, log_level, source, pyload
filter {
    if [fields][log_source] =~ /^spic\d+$/ {
        grok {
            match => { 
				"message" => "%{TIMESTAMP_ISO8601:timestamp},%{NUMBER:milliseconds} %{LOGLEVEL:log_level} %{GREEDYDATA:content}" 
				# [
                #     ".*/(?<market>[a-z]+)_(?<account>\d+)_(?<type>[a-z]+(_[a-z]+)*).*",
                #     ".*/(?<market>[a-z]+)_(?<type>[a-z]+(_[a-z]+)*).*"
                # ]
			}
			add_field => { 
				"tot_timestamp" => "%{timestamp}.%{milliseconds}"
				"log_source" => "%{[fields][log_source]}" 
				"file_path" => "%{[log][file][path]}"
			}
        }
		grok {
			match => {
				"file_path" => [
					".*/(?<market>[a-z]+)_(?<account>\d+)_(?<type>[a-z]+(_[a-z]+)*).*",
					".*/(?<market>[a-z]+)_(?<type>[a-z]+(_[a-z]+)*).*"
				]
			}
		}
        date {
            match => [ "tot_timestamp", "yyyy-MM-dd HH:mm:ss.SSS" ]
            target => "log_timestamp"
        }
		if "_grokparsefailure" in [tags] {
			drop { }
		}
		prune {
            whitelist_names => ["^log_timestamp$",  "^log_level$", "^content$", "^log_source$", "^market$", "^account$", "^type$"]
        }
        # mutate {
        #     remove_field => ["fields", "timestamp", "milliseconds", "tot_timestamp"]
        # }
    }
}

## Add your filters / logstash plugins configuration here

output {
	elasticsearch {
		hosts => "elasticsearch:9200"
		index => "logstash-%{+YYYY.MM.dd}"
		user => "logstash_internal"
		password => "${LOGSTASH_INTERNAL_PASSWORD}"
	}
	stdout {
		codec => rubydebug
	}
}